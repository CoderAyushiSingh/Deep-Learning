{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Image Segmentation & Mask R-CNN | Assignment"
      ],
      "metadata": {
        "id": "BKZOF-9TnhIj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 1: What is TensorFlow Object Detection API (TFOD2) and what are its primary components?\n",
        "\n",
        "- TensorFlow Object Detection API (TFOD2) is a high-level framework built on TensorFlow 2 that helps you build, train, fine-tune, and deploy object detection models.\n",
        "\n",
        "- Primary Components of TFOD2\n",
        "\n",
        "1. Pretrained Models (Model Zoo)\n",
        "\n",
        "    - Ready-to-use models like SSD, Faster R-CNN, EfficientDet\n",
        "\n",
        "     - Trained on large datasets (COCO, Open Images)\n",
        "\n",
        "     - Used for transfer learning to save time and computation\n",
        "\n",
        "2.  Pipeline Configuration File\n",
        "\n",
        "     - A .config file that controls everything:\n",
        "\n",
        "         - Model architecture\n",
        "\n",
        "         - Training parameters\n",
        "\n",
        "         - Dataset paths\n",
        "\n",
        "         - Optimizer & learning rate\n",
        "\n",
        "    - Basically the brain of the training process\n",
        "\n",
        "3. Dataset Preparation Tools\n",
        "\n",
        "      - Converts data into TFRecord format\n",
        "\n",
        "      - Uses label maps to assign class IDs\n",
        "\n",
        "      - Ensures data is efficiently loaded during training\n",
        "\n",
        "4. Training & Evaluation Scripts\n",
        "\n",
        "      - Scripts like model_main_tf2.py\n",
        "\n",
        "      - Used to train, evaluate, and monitor performance\n",
        "\n",
        "5. Inference & Export Utilities\n",
        "\n",
        "     - Export trained models for real-world use\n",
        "\n",
        "     - Supports deployment on CPU, GPU, TPU, mobile, and edge devices\n",
        "\n",
        "\n",
        "#Question 2: Differentiate between semantic segmentation and instance segmentation. Provide examples of where each might be used.\n",
        "\n",
        "1.  Semantic Segmentation\n",
        "\n",
        "    - Assigns a class label to every pixel in an image\n",
        "\n",
        "    - Does NOT distinguish between different objects of the same class\n",
        "\n",
        "    - All objects of the same category are treated as one region\n",
        "\n",
        "   - Example:\n",
        "\n",
        "       - In a road scene, all cars are labeled as “car”, all people as “person”\n",
        "\n",
        "       - Used in:\n",
        "\n",
        "            - Autonomous driving (road, lane, sidewalk detection)\n",
        "\n",
        "           - Medical imaging (tumor vs healthy tissue)\n",
        "\n",
        "          - Satellite image analysis (land, water, vegetation)\n",
        "\n",
        "2. Instance Segmentation\n",
        "\n",
        "    - Assigns a class label + a unique ID to each object instance\n",
        "\n",
        "    - Separates individual objects, even if they belong to the same class\n",
        "\n",
        "- Example:\n",
        "\n",
        "   - Two cars → car_1 and car_2, each with its own mask\n",
        "\n",
        "   - Used in:\n",
        "\n",
        "      - Crowd counting\n",
        "\n",
        "      - Object tracking\n",
        "\n",
        "       - Robotics & manufacturing (picking specific objects)\n",
        "\n",
        "       - Wildlife monitoring\n",
        "\n",
        "\n",
        "#Question 3: Explain the Mask R-CNN architecture. How does it extend Faster R-CNN?\n",
        "\n",
        "- Mask R-CNN is an advanced instance segmentation model that extends Faster R-CNN by adding a branch for predicting pixel-level object masks, in addition to object detection.\n",
        "\n",
        "1. Backbone Network\n",
        "\n",
        "   - Mask R-CNN uses a CNN backbone such as ResNet-50/101 combined with Feature Pyramid Network (FPN) to extract rich multi-scale feature maps from the input image.\n",
        "\n",
        "2. Region Proposal Network (RPN)\n",
        "\n",
        "    - Like Faster R-CNN, an RPN generates region proposals (RoIs) by predicting objectness scores and bounding boxes over the feature maps.\n",
        "\n",
        "3. RoIAlign (Key Improvement)\n",
        "\n",
        "    - Instead of RoIPool used in Faster R-CNN, Mask R-CNN introduces RoIAlign, which:\n",
        "\n",
        "       - Removes quantization errors\n",
        "\n",
        "       - Uses bilinear interpolation\n",
        "\n",
        "       - Preserves exact spatial alignment\n",
        "       - This is crucial for accurate pixel-level mask prediction.\n",
        "\n",
        "4. Parallel Output Heads\n",
        "\n",
        "     - For each RoI, Mask R-CNN has three parallel branches:\n",
        "\n",
        "     - Classification head → predicts object class\n",
        "\n",
        "     - Bounding box regression head → refines bounding boxes\n",
        "\n",
        "     - Mask head → predicts a binary segmentation mask for each class using a Fully Convolutional Network (FCN)\n",
        "\n",
        "     - The mask branch predicts a fixed-size mask (e.g., 28×28) for each RoI independently of classification.\n",
        "\n",
        "5. Multi-Task Loss\n",
        "\n",
        "    - The total loss is a combination of:\n",
        "\n",
        "        - Classification loss\n",
        "\n",
        "        - Bounding box regression loss\n",
        "\n",
        "        - Mask loss (pixel-wise binary cross-entropy)\n",
        "\n",
        "6.  How Mask R-CNN Extends Faster R-CNN\n",
        "\n",
        "        - Adds a mask prediction branch for instance segmentation\n",
        "\n",
        "        - Replaces RoIPool with RoIAlign for better spatial precision\n",
        "\n",
        "        - Performs detection and segmentation simultaneously\n",
        "\n",
        "        - Produces bounding boxes, class labels, and object masks\n",
        "\n",
        "#Question 4: Describe the purpose of masks in image segmentation. How are they used during training and inference?\n",
        "\n",
        "- Purpose of Masks in Image Segmentation\n",
        "\n",
        "    - In image segmentation, a mask is a pixel-level representation that indicates which pixels belong to an object or region of interest. Each pixel in a mask is labeled (usually 0 or 1, or class IDs), enabling precise localization of objects beyond bounding boxes.\n",
        "\n",
        "    - Masks are essential for tasks like semantic segmentation and instance segmentation, where understanding the exact shape and boundaries of objects is required.\n",
        "\n",
        "- Use of Masks During Training\n",
        "\n",
        "   - During training:\n",
        "\n",
        "      - Ground truth masks are provided for each object or class.\n",
        "\n",
        "      - The model predicts a mask for every detected object or pixel region.\n",
        "\n",
        "      - The predicted mask is compared with the ground truth mask.\n",
        "\n",
        "      - A pixel-wise loss function (e.g., binary cross-entropy or Dice loss) is used to measure error.\n",
        "\n",
        "      - The loss guides the network to learn accurate object boundaries and shapes.\n",
        "\n",
        "      - In instance segmentation models like Mask R-CNN, masks are trained independently for each object instance along with classification and bounding box regression.\n",
        "\n",
        "- Use of Masks During Inference\n",
        "\n",
        "   - During inference:\n",
        "\n",
        "      - The trained model predicts masks for unseen images.\n",
        "\n",
        "       - Each mask highlights the exact pixels belonging to a detected object.\n",
        "\n",
        "        - Masks are combined with class labels and bounding boxes to produce the final output.\n",
        "\n",
        "      - The result allows precise object separation, even when objects overlap.\n",
        "\n",
        "#Question 5: What are the steps involved in training a custom image segmentation model using TFOD2?\n",
        "\n",
        "- Steps to Train a Custom Image Segmentation Model Using TFOD2\n",
        "1. Dataset Collection\n",
        "\n",
        "    - Collect images relevant to the segmentation task.\n",
        "\n",
        "    - Ensure sufficient variation (lighting, scale, background, angles).\n",
        "\n",
        "2. Data Annotation\n",
        "\n",
        "    - Annotate images using tools like LabelImg or LabelMe.\n",
        "\n",
        "    - For segmentation, create pixel-wise masks for each object.\n",
        "\n",
        "    - Save annotations in supported formats (e.g., COCO, Pascal VOC, TFRecord).\n",
        "\n",
        "3. Data Preparation\n",
        "\n",
        "    - Split the dataset into training and validation sets.\n",
        "\n",
        "    - Convert annotations into TFRecord format, which TFOD2 requires.\n",
        "\n",
        "    - Create a label map (.pbtxt) defining class IDs and names.\n",
        "\n",
        "4. Model Selection\n",
        "\n",
        "    - Choose a pretrained segmentation-capable model from the TFOD2 Model Zoo (e.g., Mask R-CNN).\n",
        "\n",
        "     - Download the model checkpoint and pipeline configuration file.\n",
        "\n",
        "5. Pipeline Configuration\n",
        "\n",
        "      - Modify the pipeline.config file:\n",
        "\n",
        "      - Set number of classes\n",
        "\n",
        "      - Update paths to TFRecords and label map\n",
        "\n",
        "     - Configure batch size, learning rate, and fine-tuning checkpoint\n",
        "\n",
        "     - Enable mask prediction parameters\n",
        "\n",
        "#Question 6: Write a Python script to install TFOD2 and verify its installation by printing the available model configs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MEQSlQ_xngv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.13.0\n",
        "!pip install tf-models-official\n",
        "!pip install protobuf==3.20.3\n",
        "!pip install cython\n",
        "!pip install pillow lxml matplotlib opencv-python\n",
        "\n",
        "!git clone https://github.com/tensorflow/models.git\n",
        "\n",
        "%cd models/research\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "!cp object_detection/packages/tf2/setup.py .\n",
        "!python -m pip install .\n",
        "\n",
        "from object_detection.utils import config_util\n",
        "\n",
        "configs = config_util.get_configs_from_pipeline_file\n",
        "print(\"TFOD2 installed successfully!\")\n",
        "print(\"Configuration utility loaded:\", configs)\n"
      ],
      "metadata": {
        "id": "YLpyymVz8kgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 7: Create a Python script to load a labeled dataset (in TFRecord format) and visualize the annotation masks over the images.\n",
        "\n"
      ],
      "metadata": {
        "id": "dxIHY7HQ9p1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def parse_tfrecord(example_proto):\n",
        "    feature_description = {\n",
        "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
        "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
        "        'image/object/mask': tf.io.FixedLenFeature([], tf.string),\n",
        "    }\n",
        "\n",
        "    example = tf.io.parse_single_example(example_proto, feature_description)\n",
        "\n",
        "    image = tf.image.decode_jpeg(example['image/encoded'], channels=3)\n",
        "    image = tf.cast(image, tf.uint8)\n",
        "\n",
        "    mask = tf.io.decode_png(example['image/object/mask'], channels=1)\n",
        "    mask = tf.squeeze(mask)\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "\n",
        "tfrecord_path = \"dataset.tfrecord\"\n",
        "\n",
        "dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
        "dataset = dataset.map(parse_tfrecord)\n",
        "\n",
        "\n",
        "for image, mask in dataset.take(1):\n",
        "\n",
        "    image = image.numpy()\n",
        "    mask = mask.numpy()\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(mask, cmap=\"gray\")\n",
        "    plt.title(\"Annotation Mask\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(image)\n",
        "    plt.imshow(mask, cmap=\"jet\", alpha=0.5)\n",
        "    plt.title(\"Mask Overlay\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "OEJwkKaa-TPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 8: Using a pre-trained Mask R-CNN model, write a code snippet to perform inference on a single image and plot the predicted masks.\n"
      ],
      "metadata": {
        "id": "rFCGh5WK--eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "\n",
        "MODEL_PATH = \"exported-model/saved_model\"\n",
        "detect_fn = tf.saved_model.load(MODEL_PATH)\n",
        "\n",
        "LABEL_MAP_PATH = \"label_map.pbtxt\"\n",
        "category_index = label_map_util.create_category_index_from_labelmap(\n",
        "    LABEL_MAP_PATH, use_display_name=True\n",
        ")\n",
        "\n",
        "IMAGE_PATH = \"test.jpg\"\n",
        "\n",
        "image = tf.io.read_file(IMAGE_PATH)\n",
        "image = tf.image.decode_jpeg(image, channels=3)\n",
        "input_tensor = tf.expand_dims(image, axis=0)\n",
        "\n",
        "detections = detect_fn(input_tensor)\n",
        "\n",
        "num_detections = int(detections.pop('num_detections'))\n",
        "detections = {k: v[0, :num_detections].numpy()\n",
        "              for k, v in detections.items()}\n",
        "detections['num_detections'] = num_detections\n",
        "detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
        "\n",
        "image_np = image.numpy()\n",
        "\n",
        "viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "    image_np,\n",
        "    detections['detection_boxes'],\n",
        "    detections['detection_classes'],_]()\n"
      ],
      "metadata": {
        "id": "7R7kutsH_QX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 9: Write a Python script to evaluate a trained TFOD2 Mask R-CNN model and plot the Precision-Recall curve.\n"
      ],
      "metadata": {
        "id": "u8BX2oO0_YU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "MODEL_PATH = \"exported-model/saved_model\"\n",
        "detect_fn = tf.saved_model.load(MODEL_PATH)\n",
        "\n",
        "def parse_tfrecord(example_proto):\n",
        "    feature_description = {\n",
        "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
        "        'image/object/class/label': tf.io.VarLenFeature(tf.int64),\n",
        "    }\n",
        "\n",
        "    example = tf.io.parse_single_example(example_proto, feature_description)\n",
        "    image = tf.image.decode_jpeg(example['image/encoded'], channels=3)\n",
        "    labels = tf.sparse.to_dense(example['image/object/class/label'])\n",
        "\n",
        "    return image, labels\n",
        "\n",
        "\n",
        "dataset = tf.data.TFRecordDataset(\"val.tfrecord\")\n",
        "dataset = dataset.map(parse_tfrecord).batch(1)\n",
        "\n",
        "y_true = []\n",
        "y_scores = []\n",
        "\n",
        "for images, labels in dataset.take(50):\n",
        "    detections = detect_fn(images)\n",
        "\n",
        "    scores = detections['detection_scores'][0].numpy()\n",
        "    classes = detections['detection_classes'][0].numpy().astype(int)\n",
        "\n",
        "    for score in scores:\n",
        "        y_scores.append(score)\n",
        "        y_true.append(1)\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision)\n",
        "plt.xlabel(\"Recall\")\n"
      ],
      "metadata": {
        "id": "tOdz3LTr_gDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 10: You are working with a city surveillance team to identify illegal parking zones from street camera images. The model you built detects cars using bounding boxes, but the team reports inaccurate overlaps with sidewalks and fails in complex street scenes. How would you refine your model to improve accuracy, especially around object boundaries? What segmentation strategy and tools would you use?\n"
      ],
      "metadata": {
        "id": "c3vVPno6_sO5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1. Use Instance Segmentation Instead of Bounding Boxes\n",
        "\n",
        " - Switch from pure object detection to instance segmentation so each car is represented by a pixel-accurate mask.\n",
        "\n",
        " -  Mask R-CNN is the obvious choice:\n",
        "\n",
        "    - Gives precise object boundaries\n",
        "\n",
        "    - Separates overlapping vehicles\n",
        "\n",
        "     - Eliminates sidewalk overlap confusion\n",
        "\n",
        "     - This directly fixes the boundary problem.\n",
        "\n",
        "2. Add Semantic Segmentation for Scene Understanding\n",
        "\n",
        "- To know where illegal parking is happening, you must segment the environment, not just cars.\n",
        "\n",
        "- Use semantic segmentation to classify:\n",
        "\n",
        "   - Road\n",
        "\n",
        "   - Sidewalk\n",
        "\n",
        "   - Parking zones\n",
        "\n",
        "   - No-parking zones\n",
        "\n",
        "   - Recommended models:\n",
        "\n",
        "       - DeepLabv3+\n",
        "\n",
        "       - U-Net (lighter, faster for city deployment)\n",
        "\n",
        "3. Improve Boundary Accuracy During Training\n",
        "\n",
        "   - To sharpen object edges:\n",
        "\n",
        "       - Use high-resolution feature maps (FPN)\n",
        "\n",
        "        - Apply RoIAlign (already in Mask R-CNN)\n",
        "\n",
        "        - Use boundary-aware losses (Dice / IoU loss)\n",
        "\n",
        "        - Train with fine-grained polygon annotations, not loose boxes\n",
        "\n",
        "       - Garbage annotations = garbage boundaries. No excuse."
      ],
      "metadata": {
        "id": "ePaflH0L__1_"
      }
    }
  ]
}