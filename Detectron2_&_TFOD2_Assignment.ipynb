{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Detectron2 & TFOD2 Assignment"
      ],
      "metadata": {
        "id": "XyrOPpPX2AfE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 1: What is Detectron2 and how does it differ from previous object detection frameworks?\n",
        "\n",
        "- Detectron2 is an open-source computer vision framework developed by Facebook AI Research (FAIR). It’s built on PyTorch and is used mainly for object detection, instance segmentation, semantic segmentation, and keypoint detection.\n",
        "\n",
        "- How is Detectron2 different from previous object detection frameworks?\n",
        "\n",
        "1. Framework base\n",
        "\n",
        "   - Detectron2: Built on PyTorch → dynamic computation graph, easier debugging, more flexible.\n",
        "\n",
        "   - Older frameworks (Detectron, Faster R-CNN original, etc.): Mostly Caffe2 or TensorFlow → rigid, harder to customize.\n",
        "\n",
        "2. Modularity & design\n",
        "\n",
        "   - Detectron2 has a clean, modular architecture (backbone, neck, head are clearly separated).\n",
        "\n",
        "    - Older frameworks were monolithic — changing one thing often broke others.\n",
        "\n",
        "3. Performance & scalability\n",
        "\n",
        "    -  Detectron2 is highly optimized for:\n",
        "\n",
        "     - Multi-GPU training\n",
        "\n",
        "     - Large datasets\n",
        "\n",
        "      - Previous frameworks struggled with scaling efficiently.\n",
        "\n",
        "4. State-of-the-art models\n",
        "\n",
        "    - Detectron2 natively supports modern models like:\n",
        "\n",
        "    - Faster R-CNN\n",
        "\n",
        "    - Mask R-CNN\n",
        "\n",
        "    - RetinaNet\n",
        "\n",
        "    - Cascade R-CNN\n",
        "\n",
        "    - Panoptic FPN\n",
        "\n",
        "\n",
        "# Question 2: Explain the process and importance of data annotation when working with Detectron2.\n",
        "\n",
        "- Data annotation is the process of labeling raw images with information such as:\n",
        "\n",
        "   - Bounding boxes\n",
        "\n",
        "   - Class labels\n",
        "\n",
        "    - Segmentation masks\n",
        "\n",
        "    - Keypoints\n",
        "\n",
        "1. Define the task\n",
        "\n",
        "   - First, decide what type of problem you’re solving:\n",
        "\n",
        "   - Object detection → bounding boxes\n",
        "\n",
        "    - Instance segmentation → pixel-level masks\n",
        "\n",
        "    - Keypoint detection → landmarks (eyes, joints, etc.)\n",
        "\n",
        "    - Detectron2 needs different annotation formats for each task.\n",
        "\n",
        "2. Choose an annotation format\n",
        "\n",
        "    - Detectron2 mainly supports:\n",
        "\n",
        "    - COCO format (most common)\n",
        "\n",
        "    - Pascal VOC (with conversion)\n",
        "\n",
        "    - Custom datasets (registered manually)\n",
        "\n",
        "3. Annotate the data\n",
        "\n",
        "    - Use annotation tools like:\n",
        "\n",
        "     - LabelImg (bounding boxes)\n",
        "\n",
        "     - CVAT\n",
        "\n",
        "     - Roboflow\n",
        "\n",
        "    - LabelMe\n",
        "\n",
        "#Question 3: Describe the steps involved in training a custom object detection model using Detectron2.\n",
        "\n",
        "1. Install and set up Detectron2\n",
        "\n",
        "    - Install PyTorch (CUDA version if GPU is available)\n",
        "\n",
        "    - Install Detectron2 compatible with your PyTorch version\n",
        "\n",
        "    - This ensures the environment is ready for training.\n",
        "\n",
        "2. Prepare and annotate the dataset\n",
        "\n",
        "    - Collect relevant images\n",
        "\n",
        "    - Annotate objects using bounding boxes (COCO format preferred)\n",
        "\n",
        "    - Assign correct class labels\n",
        "\n",
        "    - Split dataset into training and validation sets\n",
        "\n",
        "    - Detectron2 relies on high-quality labeled data.\n",
        "\n",
        "3. Register the dataset\n",
        "\n",
        "   - Register custom datasets using DatasetCatalog\n",
        "\n",
        "    - Define class names using MetadataCatalog\n",
        "\n",
        "    - This step allows Detectron2 to recognize your custom data.\n",
        "\n",
        "4. Choose a base model (pre-trained weights)\n",
        "\n",
        "    - Select a model from Detectron2 Model Zoo (e.g., Faster R-CNN, RetinaNet)\n",
        "\n",
        "    - Load pre-trained weights (usually trained on COCO)\n",
        "\n",
        "    - This speeds up training and improves accuracy via transfer learning.\n",
        "\n",
        "\n",
        "#Question 4: What are evaluation curves in Detectron2, and how are metrics like mAP and IoU interpreted?\n",
        "\n",
        "- Evaluation curves are graphical representations that show how well an object detection model performs as detection confidence thresholds change. In Detectron2, evaluation is usually done using COCO evaluation protocol, which generates:\n",
        "\n",
        "   - Precision–Recall (PR) curves\n",
        "\n",
        "    - Metrics like mAP at different IoU thresholds\n",
        "\n",
        "- Key Evaluation Curves\n",
        "1. Precision–Recall (PR) Curve\n",
        "\n",
        "   - Precision = Correct detections / Total detections\n",
        "\n",
        "   - Recall = Correct detections / Total ground-truth objects\n",
        "\n",
        "- The PR curve plots:\n",
        "\n",
        "    - Precision on Y-axis\n",
        "\n",
        "    - Recall on X-axis\n",
        "- How to interpret mAP and IoU together\n",
        "\n",
        "    - High IoU + high mAP → Excellent detection and localization\n",
        "\n",
        "    - High mAP, low IoU → Correct objects detected, poor box alignment\n",
        "\n",
        "    - Low mAP, high IoU → Accurate boxes but missing many objects\n",
        "\n",
        "    - Low both → Model is bad (no sugarcoating)\n",
        "\n",
        "#Question 5: Compare Detectron2 and TFOD2 in terms of features, performance, and ease of use.\n",
        "\n",
        "1. Framework & Backend\n",
        "\n",
        "   - Detectron2:\n",
        "        - Built on PyTorch. Dynamic computation graph, easier debugging, very research-friendly.\n",
        "\n",
        "   - TFOD2:\n",
        "        - Built on TensorFlow 2. Supports eager execution but still feels heavier.\n",
        "\n",
        "2. Performance\n",
        "\n",
        "   - Detectron2\n",
        "\n",
        "      - Optimized for multi-GPU training\n",
        "\n",
        "      - Faster experimentation\n",
        "\n",
        "      - Strong COCO benchmark performance\n",
        "\n",
        "   - TFOD2\n",
        "\n",
        "       - Stable but often slower to configure and train\n",
        "\n",
        "       - Performance depends heavily on pipeline tuning\n",
        "\n",
        "3. Ease of Use\n",
        "\n",
        "   - Detectron2\n",
        "\n",
        "      - Clean API\n",
        "\n",
        "       - Minimal boilerplate\n",
        "\n",
        "        - Steep learning curve at first, but smooth once understood\n",
        "\n",
        "   - TFOD2\n",
        "\n",
        "       - Heavy configuration files\n",
        "\n",
        "        - Complex directory structure\n",
        "\n",
        "        - Beginner-friendly tutorials but messy customization       \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fSO5eSAP2KQq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6: Write Python code to install Detectron2 and verify the installation.\n"
      ],
      "metadata": {
        "id": "pfsWkgZbrZ1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 \\\n",
        "--index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "\n",
        "!pip install detectron2 \\\n",
        "-f https://dl.fbaipublicfiles.com/detectron2/wheels/cu118/torch2.0/index.html\n",
        "\n",
        "\n",
        "import torch\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "\n",
        "setup_logger()\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"Detectron2 version:\", detectron2.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n"
      ],
      "metadata": {
        "id": "inIQItISPojW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 7: Annotate a dataset using any tool of your choice and convert the annotations to COCO format for Detectron2.\n",
        ""
      ],
      "metadata": {
        "id": "gEH3sdHhP8Eb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "\n",
        "XML_DIR = \"annotations\"\n",
        "IMG_DIR = \"images\"\n",
        "OUTPUT_JSON = \"coco_annotations.json\"\n",
        "\n",
        "coco = {\n",
        "    \"images\": [],\n",
        "    \"annotations\": [],\n",
        "    \"categories\": []\n",
        "}\n",
        "\n",
        "category_map = {}\n",
        "ann_id = 1\n",
        "img_id = 1\n",
        "\n",
        "for xml_file in os.listdir(XML_DIR):\n",
        "    if not xml_file.endswith(\".xml\"):\n",
        "        continue\n",
        "\n",
        "    tree = ET.parse(os.path.join(XML_DIR, xml_file))\n",
        "    root = tree.getroot()\n",
        "\n",
        "    filename = root.find(\"filename\").text\n",
        "    width = int(root.find(\"size/width\").text)\n",
        "    height = int(root.find(\"size/height\").text)\n",
        "\n",
        "    coco[\"images\"].append({\n",
        "        \"id\": img_id,\n",
        "        \"file_name\": filename,\n",
        "        \"width\": width,\n",
        "        \"height\": height\n",
        "    })\n",
        "\n",
        "    for obj in root.findall(\"object\"):\n",
        "        label = obj.find(\"name\").text\n",
        "\n",
        "        if label not in category_map:\n",
        "            category_map[label] = len(category_map) + 1\n",
        "            coco[\"categories\"].append({\n",
        "                \"id\": category_map[label],\n",
        "                \"name\": label\n",
        "            })\n",
        "\n",
        "        bbox = obj.find(\"bndbox\")\n",
        "        xmin = int(bbox.find(\"xmin\").text)\n",
        "        ymin = int(bbox.find(\"ymin\").text)\n",
        "        xmax = int(bbox.find(\"xmax\").text)\n",
        "        ymax = int(bbox.find(\"ymax\").text)\n",
        "\n",
        "        coco[\"annotations\"].append({\n",
        "            \"id\": ann_id,\n",
        "            \"image_id\": img_id,\n",
        "            \"category_id\": category_map[label],\n",
        "            \"bbox\": [xmin, ymin, xmax - xmin, ymax - ymin],\n",
        "            \"area\": (xmax - xmin) * (ymax - ymin),\n",
        "            \"iscrowd\": 0\n",
        "        })\n",
        "        ann_id += 1\n",
        "\n",
        "    img_id += 1\n",
        "\n",
        "with open(OUTPUT_JSON, \"w\") as f:\n",
        "    json.dump(coco, f, indent=4)\n",
        "\n",
        "register_coco_instances(\n",
        "    \"custom_dataset\",\n",
        "    {},\n",
        "    OUTPUT_JSON,\n",
        "    IMG_DIR\n",
        ")\n"
      ],
      "metadata": {
        "id": "q02LFnlHriOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 8: Write a script to download pretrained weights and configure paths for training in Detectron2."
      ],
      "metadata": {
        "id": "wVkJPaVZTQlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "import os\n",
        "\n",
        "cfg = get_cfg()\n",
        "\n",
        "cfg.merge_from_file(\n",
        "    model_zoo.get_config_file(\n",
        "        \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n",
        "    )\n",
        ")\n",
        "\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\n",
        "    \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n",
        ")\n",
        "\n",
        "cfg.DATASETS.TRAIN = (\"custom_dataset\",)\n",
        "cfg.DATASETS.TEST = ()\n",
        "\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "cfg.SOLVER.MAX_ITER = 3000\n",
        "cfg.SOLVER.STEPS = []\n",
        "cfg.SOLVER.CHECKPOINT_PERIOD = 500\n",
        "\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3\n",
        "\n",
        "cfg.OUTPUT_DIR = \"./output\"\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "with open(os.path.join(cfg.OUTPUT_DIR, \"config.yaml\"), \"w\") as f:\n",
        "    f.write(cfg.dump())\n"
      ],
      "metadata": {
        "id": "4wVGYjEMTktI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 9: Show the steps and code to run inference using a trained Detectron2 model on a new image.\n"
      ],
      "metadata": {
        "id": "wJnViKr5UBac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog\n",
        "from detectron2 import model_zoo\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(\n",
        "    model_zoo.get_config_file(\n",
        "        \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n",
        "    )\n",
        ")\n",
        "\n",
        "cfg.MODEL.WEIGHTS = \"output/model_final.pth\"\n",
        "\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 3\n",
        "cfg.MODEL.DEVICE = \"cpu\"\n",
        "\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "image = cv2.imread(\"test.jpg\")\n",
        "\n",
        "outputs = predictor(image)\n",
        "\n",
        "metadata = MetadataCatalog.get(\"custom_dataset\")\n",
        "v = Visualizer(image[:, :, ::-1], metadata=metadata, scale=1.0)\n",
        "result = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "\n",
        "cv2.imshow(\"Inference Output\", result.get_image()[:, :, ::-1])\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "id": "aPivuAnlUG1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 10: You are assigned to build a wildlife monitoring system to detect and track different animal species in a forest using Detectron2. Describe the end-to-end pipeline from data collection to deploying the model, and how you would handle challenges like occlusion or nighttime detection.\n"
      ],
      "metadata": {
        "id": "ZMdm2gjDUnBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Wildlife Monitoring System using Detectron2 – End-to-End Pipeline**\n",
        "\n",
        "1. Data is collected using camera traps and drones placed in forest areas to capture animals under different conditions such as daytime, nighttime, and dense vegetation. The collected images and videos are annotated using tools like CVAT or LabelImg by labeling animal species with bounding boxes or instance masks and converting annotations to COCO format.\n",
        "\n",
        "2. The data is preprocessed and augmented using techniques like resizing, flipping, brightness adjustment, motion blur, and synthetic occlusion to improve model robustness. A Detectron2 model such as Faster R-CNN or Mask R-CNN is fine-tuned using pretrained COCO weights. Model performance is evaluated using metrics like mAP and IoU, with special focus on occluded and low-light samples.\n",
        "\n",
        "3. Occlusion is handled by training on partially visible animals, using instance segmentation (Mask R-CNN), multi-scale features, and temporal information from video frames. Nighttime detection is improved by training on infrared or thermal images, applying low-light augmentation, and using image enhancement techniques.\n",
        "\n",
        "4. For video streams, detected animals are tracked using algorithms like DeepSORT or ByteTrack. The trained model is deployed on edge devices or central servers for real-time monitoring, and continuous retraining is performed using newly collected data."
      ],
      "metadata": {
        "id": "bfOY9G7HWJRR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ISSbT0eAUx4n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}